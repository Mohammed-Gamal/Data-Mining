{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center'>\n",
    "  <h2 style=\"color:Orange\">\n",
    "    Fully Automated Code\n",
    "    <img\n",
    "      src=\"https://cdn-icons-png.flaticon.com/512/2825/2825945.png\"\n",
    "      style=\"width:60px;margin-right: 10px\"\n",
    "    />\n",
    "  </h2>\n",
    "  <p>All you got to do is to change only the dataset name and the desired target name.</p>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4024, 16)\n",
      "      Age   Race Marital Status T Stage  N Stage 6th Stage  \\\n",
      "0      68  White        Married       T1      N1       IIA   \n",
      "1      50  White        Married       T2      N2      IIIA   \n",
      "2      58  White       Divorced       T3      N3      IIIC   \n",
      "3      58  White        Married       T1      N1       IIA   \n",
      "4      47  White        Married       T2      N1       IIB   \n",
      "...   ...    ...            ...      ...     ...       ...   \n",
      "4019   62  Other        Married       T1      N1       IIA   \n",
      "4020   56  White       Divorced       T2      N2      IIIA   \n",
      "4021   68  White        Married       T2      N1       IIB   \n",
      "4022   58  Black       Divorced       T2      N1       IIB   \n",
      "4023   46  White        Married       T2      N1       IIB   \n",
      "\n",
      "                  differentiate  Grade   A Stage  Tumor Size Estrogen Status  \\\n",
      "0         Poorly differentiated      3  Regional           4        Positive   \n",
      "1     Moderately differentiated      2  Regional          35        Positive   \n",
      "2     Moderately differentiated      2  Regional          63        Positive   \n",
      "3         Poorly differentiated      3  Regional          18        Positive   \n",
      "4         Poorly differentiated      3  Regional          41        Positive   \n",
      "...                         ...    ...       ...         ...             ...   \n",
      "4019  Moderately differentiated      2  Regional           9        Positive   \n",
      "4020  Moderately differentiated      2  Regional          46        Positive   \n",
      "4021  Moderately differentiated      2  Regional          22        Positive   \n",
      "4022  Moderately differentiated      2  Regional          44        Positive   \n",
      "4023  Moderately differentiated      2  Regional          30        Positive   \n",
      "\n",
      "     Progesterone Status  Regional Node Examined  Reginol Node Positive  \\\n",
      "0               Positive                      24                      1   \n",
      "1               Positive                      14                      5   \n",
      "2               Positive                      14                      7   \n",
      "3               Positive                       2                      1   \n",
      "4               Positive                       3                      1   \n",
      "...                  ...                     ...                    ...   \n",
      "4019            Positive                       1                      1   \n",
      "4020            Positive                      14                      8   \n",
      "4021            Negative                      11                      3   \n",
      "4022            Positive                      11                      1   \n",
      "4023            Positive                       7                      2   \n",
      "\n",
      "      Survival Months Status  \n",
      "0                  60  Alive  \n",
      "1                  62  Alive  \n",
      "2                  75  Alive  \n",
      "3                  84  Alive  \n",
      "4                  50  Alive  \n",
      "...               ...    ...  \n",
      "4019               49  Alive  \n",
      "4020               69  Alive  \n",
      "4021               69  Alive  \n",
      "4022               72  Alive  \n",
      "4023              100  Alive  \n",
      "\n",
      "[4024 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Data_set = pd.read_csv(\"Breast_Cancer.csv\", delimiter=\",\")\n",
    "print(Data_set.shape)  # 4024 rows, 16 columns\n",
    "print(Data_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, retrieving and formatting the target/goal (i.e., differentiate feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alive', 'Dead']\n"
     ]
    }
   ],
   "source": [
    "target_name = 'Status'\n",
    "\n",
    "target = Data_set[target_name].tolist()\n",
    "target = list(set(target))  # we used a set to retrieve the unique names only\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the desired features (i.e., columns) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024 rows, 16 columns\n",
      "\n",
      "Index(['Age', 'Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage',\n",
      "       'differentiate', 'Grade', 'A Stage', 'Tumor Size', 'Estrogen Status',\n",
      "       'Progesterone Status', 'Regional Node Examined',\n",
      "       'Reginol Node Positive', 'Survival Months'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'{Data_set.shape[0]} rows, {Data_set.shape[1]} columns\\n')  # 4024, 16\n",
    "\n",
    "Features_names = Data_set.columns[0:Data_set.shape[1]-1]  # all except the 'Status' column\n",
    "print(Features_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the values of the retrieved columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68 'White' 'Married' ... 24 1 60]\n",
      " [50 'White' 'Married' ... 14 5 62]\n",
      " [58 'White' 'Divorced' ... 14 7 75]\n",
      " ...\n",
      " [68 'White' 'Married' ... 11 3 69]\n",
      " [58 'Black' 'Divorced' ... 11 1 72]\n",
      " [46 'White' 'Married' ... 7 2 100]]\n"
     ]
    }
   ],
   "source": [
    "X = Data_set[Features_names].values\n",
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Step (Categorical data to numeric data, for distance functions)\n",
    "##### LabelEncoder() is used to encode categorical data as numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Race → object\n",
      "['Black', 'White', 'Other']\n",
      "\n",
      "2: Marital Status → object\n",
      "['Widowed', 'Single ', 'Married', 'Separated', 'Divorced']\n",
      "\n",
      "3: T Stage  → object\n",
      "['T4', 'T2', 'T3', 'T1']\n",
      "\n",
      "4: N Stage → object\n",
      "['N1', 'N3', 'N2']\n",
      "\n",
      "5: 6th Stage → object\n",
      "['IIB', 'IIIC', 'IIA', 'IIIA', 'IIIB']\n",
      "\n",
      "6: differentiate → object\n",
      "['Well differentiated', 'Undifferentiated', 'Moderately differentiated', 'Poorly differentiated']\n",
      "\n",
      "8: A Stage → object\n",
      "['Regional', 'Distant']\n",
      "\n",
      "10: Estrogen Status → object\n",
      "['Negative', 'Positive']\n",
      "\n",
      "11: Progesterone Status → object\n",
      "['Negative', 'Positive']\n",
      "\n",
      "[[68 2 1 ... 24 1 60]\n",
      " [50 2 1 ... 14 5 62]\n",
      " [58 2 0 ... 14 7 75]\n",
      " ...\n",
      " [68 2 1 ... 11 3 69]\n",
      " [58 0 0 ... 11 1 72]\n",
      " [46 2 1 ... 7 2 100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "for index, feature in enumerate(Features_names):\n",
    "\n",
    "  # Ignore numerical features\n",
    "  if (Data_set.dtypes[feature] == 'object'):\n",
    "    print(f'{index}: {feature} → {Data_set.dtypes[feature]}')\n",
    "\n",
    "    # 1) Get the unique values\n",
    "    label = Data_set[feature].tolist()\n",
    "    label = list(set(label))\n",
    "    print(label, end='\\n\\n')\n",
    "\n",
    "    # 2) Convert to numerical\n",
    "    label_numeric = preprocessing.LabelEncoder()\n",
    "    label_numeric.fit(label)\n",
    "    X[:, index] = label_numeric.transform(X[:, index])\n",
    "\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Alive\n",
      "1       Alive\n",
      "2       Alive\n",
      "3       Alive\n",
      "4       Alive\n",
      "        ...  \n",
      "4019    Alive\n",
      "4020    Alive\n",
      "4021    Alive\n",
      "4022    Alive\n",
      "4023    Alive\n",
      "Name: Status, Length: 4024, dtype: object\n",
      "(4024, 16)\n",
      "(3219, 15)\n",
      "(805, 15)\n",
      "(3219,)\n",
      "(805,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = Data_set[target_name]  # Terget/Goal\n",
    "print(Y)\n",
    "\n",
    "# Dimensions of the dataset\n",
    "print(Data_set.shape)  # 4024, 16\n",
    "\n",
    "# Split the data into 80% for training and 20% for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)  # 3 samples per iteration\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbors) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "(805,)\n",
      "\n",
      "Predicted by KNN:\n",
      " ['Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive'\n",
      " 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive']\n",
      "\n",
      "KNN confusion matrix:\n",
      " [[685   7]\n",
      " [ 65  48]]\n",
      "\n",
      "KNN Accuracy:  0.9105590062111801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "# Choosing the best value for n. 'sqrt(n)'\n",
    "n = int(sqrt(Data_set.shape[0]))\n",
    "print(n)\n",
    "\n",
    "# Apply the KNN classifier with n neighbors\n",
    "neigh = KNeighborsClassifier(n_neighbors=n)  \n",
    "\n",
    "\n",
    "# Train the Model using Training Sets (i.e. Classified data)\n",
    "neigh.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# Do prediction on the testing set\n",
    "predicted = neigh.predict(X_test)\n",
    "print(predicted.shape)\n",
    "print(\"\\nPredicted by KNN:\\n\", predicted)\n",
    "\n",
    "\n",
    "# Compare the predicted results with the predefined data (i.e., Accuracy)\n",
    "results = metrics.confusion_matrix(Y_test, predicted)\n",
    "print(\"\\nKNN confusion matrix:\\n\", results)\n",
    "\n",
    "print(\"\\nKNN Accuracy: \", metrics.accuracy_score(Y_test, predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805,)\n",
      "\n",
      "Predicted by Naive Bayes:\n",
      " ['Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead'\n",
      " 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Dead' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Dead' 'Alive'\n",
      " 'Dead' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Dead' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Dead' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Dead' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive' 'Alive'\n",
      " 'Dead' 'Dead' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Dead' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Dead' 'Alive'\n",
      " 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Alive' 'Dead' 'Alive' 'Alive'\n",
      " 'Alive' 'Dead' 'Alive' 'Alive']\n",
      "\n",
      "Naive Bayes confusion matrix:\n",
      " [[603  89]\n",
      " [ 54  59]]\n",
      "\n",
      "Naive Bayes Accuracy:  0.822360248447205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a GaussianNB Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the Model using Training Sets (i.e. Classified data)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# Do prediction on the testing set\n",
    "predicted = model.predict(X_test)\n",
    "print(predicted.shape)\n",
    "print(\"\\nPredicted by Naive Bayes:\\n\", predicted)\n",
    "\n",
    "\n",
    "# Compare the predicted results with the predefined data (i.e., Accuracy)\n",
    "results = metrics.confusion_matrix(Y_test, predicted)\n",
    "print(\"\\nNaive Bayes confusion matrix:\\n\", results)\n",
    "\n",
    "print(\"\\nNaive Bayes Accuracy: \", metrics.accuracy_score(Y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
